{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036382e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e301c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260eb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_unlink(path_to_zip_file, directory_to_extract_to):\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "    os.unlink(path_to_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf66c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrangle:\n",
    "    def __init__(self, config_file_location):\n",
    "        self._get_config(config_file_location)\n",
    "        \n",
    "    def _get_config(self, mconfig):\n",
    "\n",
    "        with open(mconfig, 'r') as rfile:\n",
    "            cfg = yaml.safe_load(rfile)\n",
    "            #print(cfg)\n",
    "        try:\n",
    "            # server_name = 'USGS-DDS'\n",
    "            self.username = cfg['username']\n",
    "            self.password = cfg['password']\n",
    "            self.site = cfg['site']\n",
    "            self.url_path = cfg['url']\n",
    "            self.out_loc = cfg['output_location']\n",
    "            print(self.out_loc)\n",
    "            self.output_name = cfg['outname']\n",
    "\n",
    "        except ConnectionError:\n",
    "            print('connection error occured')\n",
    "            raise\n",
    "\n",
    "    def get_all_links(self):\n",
    "    \n",
    "        soup = BeautifulSoup(requests.get(self.url_path, auth = HTTPBasicAuth(self.username, self.password)).text)\n",
    "        schooltable = soup.find('table')\n",
    "        my_links = schooltable.find_all(\"a\", {\"class\": \"linkUrl\"})\n",
    "        for link in my_links:\n",
    "            yield(link['href'])\n",
    "    \n",
    "    def download(self, url_link):\n",
    "\n",
    "        if not os.path.exists(self.out_loc):\n",
    "            os.makedirs(self.out_loc)  # create folder if it does not exist\n",
    "        \n",
    "        url =f'{self.site}{url_link}'\n",
    "\n",
    "        filename = url.split('/')[-1].replace(\" \", \"_\")  # be careful with file names\n",
    "        file_path = os.path.join(self.out_loc, filename)\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        r = requests.get(url, stream=True, auth = HTTPBasicAuth(self.username, self.password))\n",
    "        if r.ok:\n",
    "            print(\"saving to\", os.path.abspath(file_path))\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 8):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "        else:  # HTTP status code 4XX/5XX\n",
    "            print(\"Download failed: status code {}\\n{}\".format(r.status_code, r.text))\n",
    "    \n",
    "    def unzip_unlink(self, relatiive_path):\n",
    "        full_path = f'{self.out_loc}{relative_path}'\n",
    "        print(f'UNLINK:{full_path}')\n",
    "        unzip_unlink(full_path, self.out_loc)\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bd3068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wsefs/tony/sdata/\n",
      "/highvolume/eviirs/Global_LST/expedited/VIIRS/2022/comp_293/LS_eVSE_TEMP.2022.284-293.1KM.COMPRES.006.2022294053847.zip\n",
      "https://dds.cr.usgs.gov/highvolume/eviirs/Global_LST/expedited/VIIRS/2022/comp_293/LS_eVSE_TEMP.2022.284-293.1KM.COMPRES.006.2022294053847.zip\n",
      "saving to /wsefs/tony/sdata/LS_eVSE_TEMP.2022.284-293.1KM.COMPRES.006.2022294053847.zip\n",
      "COMPLETED /highvolume/eviirs/Global_LST/expedited/VIIRS/2022/comp_293/LS_eVSE_TEMP.2022.284-293.1KM.COMPRES.006.2022294053847.zip\n",
      "UNLINK:/wsefs/tony/sdata/LS_eVSE_TEMP.2022.284-293.1KM.COMPRES.006.2022294053847.zip\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    config = '/home/ec2-user/.config/wrangle.yaml'\n",
    "    wr = Wrangle(config)\n",
    "    \n",
    "    for link in wr.get_all_links():\n",
    "        if link.endswith('.zip'):\n",
    "            print(link)\n",
    "            wr.download(link)\n",
    "            print('COMPLETED', link)\n",
    "            just_file = link.split('/')[-1]\n",
    "            # full_path = f'{out_loc}{just_file}'\n",
    "            # print(full_path)\n",
    "            relative_path = just_file\n",
    "            wr.unzip_unlink(relative_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb711bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f6ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
